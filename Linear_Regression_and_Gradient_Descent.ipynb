{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexsuakim/Machine-Learning/blob/main/Linear_Regression_and_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6tFiSLZY_i6"
      },
      "source": [
        "###Linear Regression and Gradient Descent\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAIxvgcQY_i8",
        "outputId": "f98e022b-3322-4615-c94c-382a1065e5a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\sua\\anaconda3\\lib\\site-packages (1.21.5)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\sua\\anaconda3\\lib\\site-packages (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\sua\\anaconda3\\lib\\site-packages (from matplotlib) (1.21.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\sua\\anaconda3\\lib\\site-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sua\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\sua\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\sua\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sua\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sua\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sua\\anaconda3\\lib\\site-packages (from matplotlib) (9.0.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\sua\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: patchify in c:\\users\\sua\\anaconda3\\lib\\site-packages (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1 in c:\\users\\sua\\anaconda3\\lib\\site-packages (from patchify) (1.21.5)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import sys\n",
        "! pip install numpy\n",
        "import numpy as np\n",
        "! pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import os\n",
        "from matplotlib.pyplot import imread\n",
        "! pip install patchify\n",
        "from patchify import patchify\n",
        "\n",
        "np.random.seed(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stcNiN0GY_jJ"
      },
      "source": [
        "---\n",
        "Implement multiple target **batch** linear regression with mean squared loss,\n",
        "\n",
        "$$\\mathcal{L} = \\frac{1}{2 m} \\sum_{i = 0}^{m} \\mid \\mid x_i\\theta - y_i \\mid \\mid^2$$\n",
        "\n",
        "- $x \\in \\mathbb{R}^{m}$ is the vector directly representing input features from the provided dataset. Every element of it is a single training example.\n",
        "- $X \\in \\mathbb{R}^{m \\times n}$ is the constructed feature matrix (e.g. polynomial features) used for learning. Each row of $X$ is a single training example.\n",
        "- $\\theta$ is our parameters.\n",
        "- $y \\in \\mathbb{R}^{m}$ is a matrix of the target values we're trying to estimate for each row of $X$. Each row $i$ of $X$ corresponds to row $i$ of $Y$.\n",
        "- $m$ is the number of training examples.\n",
        "- $n$ is the dimensionality of one training example.\n",
        "\n",
        "Linear regression is the mapping from $\\mathbb{R}^n \\rightarrow \\mathbb{R}$, where they're trying to predict a single scalar value.\n",
        "\n",
        "---\n",
        "First, we load the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUqHA1omY_jK"
      },
      "outputs": [],
      "source": [
        "x_train, _, y_train, _ = np.load(\"./data_regression.npy\")\n",
        "plt.plot(x_train,y_train,'o')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title(\"Training data\")\n",
        "plt.ylim([-1,3])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw14rL_rY_jK"
      },
      "source": [
        "It is obvious that it is not a good idea to perform linear regression directly on the input feature `x`. We need to add polynomial features. Lets construct an appropriate feature vector.\n",
        "\n",
        "---\n",
        "Complete the `get_polynomial_features` function with the following specifications.\n",
        "* Input1: an array `x` of shape $(m,1)$.\n",
        "* Input2: `degree` of the polynomial (integer greater than or equal to one).\n",
        "* Output: matrix of shape $(m,degree+1)$ consisting of horizontally concatenated polynomial terms.\n",
        "* Output: the first column of output matrix should be all ones.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oaIoYlfY_jK"
      },
      "outputs": [],
      "source": [
        "def get_polynomial_features(x,degree=5):\n",
        "    FeatureArray = np.empty((len(x), degree + 1), float)\n",
        "    for i in range (len(x)):\n",
        "        for j in range (degree + 1):\n",
        "            FeatureArray[i][j] = x[i]**j\n",
        "    return FeatureArray\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "# get polynomial features\n",
        "X_train = get_polynomial_features(x_train,degree=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gskj-bHkY_jK"
      },
      "source": [
        "Let us implement gradient descent to find the optimal $\\theta$.\n",
        "\n",
        "\n",
        "---\n",
        "Write a function $initialise\\_parameters(n) = \\theta$, where $\\theta$ is the parameters we will use for linear regression $X\\theta = Y$ for $X \\in \\mathbb{R}^{m \\times n}, Y \\in \\mathbb{R}^{m}$.\n",
        "\n",
        "The values of $\\theta$ should be randomly generated. You will be judged on whether the matrix $\\theta$ is correctly constructed for this problem.\n",
        "\n",
        "$\\theta$ should be an array of length $n$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "6jBlVyK0Y_jK"
      },
      "outputs": [],
      "source": [
        "def initialise_parameters(n):\n",
        "    theta = []\n",
        "    for i in range (n):\n",
        "        theta.append(np.random.uniform(-10,10))\n",
        "    return theta\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "# initialize theta\n",
        "theta = initialise_parameters(X_train.shape[1])\n",
        "print(theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBlXiKeCY_jL"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "Implement a function $ms\\_error(X, \\theta, y) = err$, which gives the **mean** squared error over all $m$ training examples.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCsETG6FY_jL"
      },
      "outputs": [],
      "source": [
        "def ms_error(X, theta, y):\n",
        "\n",
        "    return np.transpose(y - X @ theta) @ (y - X @ theta) / len(y)\n",
        "    pass\n",
        "\n",
        "print(ms_error(X_train, theta, y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VON7I4SBY_jL"
      },
      "source": [
        "Implement $grad(X, \\theta, Y) = g$, a function that returns the average gradient ($\\partial \\mathcal{L}/\\partial {\\theta}$) across all the training examples $x_i \\in \\mathbb{R}^{1 \\times n}$.\n",
        "\n",
        "---\n",
        "\n",
        "- The gradient should be an array with same length as $\\theta$.\n",
        "- https://www.sharpsightlabs.com/blog/numpy-sum/\n",
        "- https://docs.scipy.org/doc/numpy/reference/generated/numpy.multiply.html\n",
        "- https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUxYfet0Y_jL"
      },
      "outputs": [],
      "source": [
        "def grad(X, theta, Y):\n",
        "    #gradient of dL/d(theta) can be mathematically reduced to the following equation\n",
        "    gradient = (-2 * np.transpose(Y) @ X + 2 * np.transpose(theta) @ np.transpose(X) @ X) / len(Y)\n",
        "    return gradient\n",
        "\n",
        "\n",
        "    pass\n",
        "\n",
        "print(grad(X_train, theta, y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J4nIzDTY_jL"
      },
      "source": [
        "---\n",
        "Implement $batch\\_descent(X, Y, iterations, learning\\_rate) = \\theta, L$, a function which implements batch gradient descent returning $\\theta$ (parameters which estimate $Y$ from $X$), and $L$.\n",
        "\n",
        "$iterations$ is the number of gradient descent iterations to be performed.\n",
        "\n",
        "$learning\\_rate$ is, of course, the learning rate.\n",
        "\n",
        "$L$ is a matrix recording the mean squared error at every iteration of gradient descent. It will be an array of length $iterations$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGuUzfv_Y_jL"
      },
      "outputs": [],
      "source": [
        "def batch_descent(X, Y, iterations, learning_rate):\n",
        "\n",
        "    theta = initialise_parameters(X.shape[1])\n",
        "    L = []\n",
        "    for i in range (iterations):\n",
        "        theta = theta - 0.5 * grad(X, theta, Y)\n",
        "        L.append(ms_error(X, theta, Y))\n",
        "    return theta, L\n",
        "    pass\n",
        "\n",
        "\n",
        "new_theta, L = batch_descent(X_train, y_train, 5000, 0.5)\n",
        "plt.plot(L)\n",
        "plt.title('Mean Squared Error vs Iterations')\n",
        "plt.show()\n",
        "print('New Theta: \\n', new_theta)\n",
        "print('\\nFinal Mean Squared Error: \\n', ms_error(X_train, new_theta, y_train))\n",
        "\n",
        "def get_prediction(X,theta):\n",
        "    pred = X@theta\n",
        "    return pred\n",
        "\n",
        "x_fit = np.linspace(-0.7, 0.8, 1000)\n",
        "X_fit = get_polynomial_features(x_fit,degree=2)\n",
        "pred_y_train = get_prediction(X_fit,new_theta)\n",
        "\n",
        "# plot results\n",
        "plt.plot(x_train,y_train,'o',label='data point')\n",
        "plt.plot(x_fit,pred_y_train,label='fitting result')\n",
        "plt.legend()\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('show fitting result')\n",
        "plt.ylim([-1,3])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}